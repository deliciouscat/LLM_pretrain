{"storage":"container","type":"python","detailedtype":"pytorch_mnist","author":"101553155721587597963_yqrre_google","name":"LLM_pretrain","description":"integrate SotA pretraining methods in Korean LLM, and trying out my ideas.","date":"2023/10/2 7:23:14","plugins":{"goorm.plugin.python":[{"plugin.python.compiler_type":"python3","plugin.python.main":"index","plugin.python.source_path":"","plugin.python.run_on":"console","plugin.python.run_option":"","plugin.python.log_path":"server.log"}]},"is_user_plugin":false,"author_email":"sunmo009@gmail.com","author_name":"구선모","ignore_patterns":[],"project_domain":[{"id":"101553155721587597963_yqrre_google","url":"llm-pretrain-geuei.run.goorm.site","port":"80"}],"visibility":2}